{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a55ca16",
   "metadata": {},
   "source": [
    "# For sample learing\n",
    "## 1. single cell agent, custom environment, custom loop with Q table (value based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c7749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from envs.minimumEnv_singleCell import minimumColonyEnv\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf076cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning method\n",
    "# implement Q-table\n",
    "Q = {}\n",
    "for n in [0,1,2]:\n",
    "    for p in [0,1,2]:\n",
    "        Q[(n,p)] = [0,0,0]  # 3 actions\n",
    "\n",
    "alpha = 0.1  # learning rate\n",
    "gamma = 0.9  # discount factor\n",
    "epsilon = 0.1  # exploration rate\n",
    "\n",
    "env = minimumColonyEnv()\n",
    "num_episodes = 500\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    for t in range(20):\n",
    "        # epsilon-greedy action selection\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.randint(0, 2)  # explore\n",
    "        else:\n",
    "            action = np.argmax(Q[state])  # exploit\n",
    "            \n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # Q-learning update\n",
    "        #print(state, \" : \", Q)\n",
    "        best_next = max(Q[next_state])\n",
    "        Q[state][action] = Q[state][action] + alpha * (reward + gamma * best_next - Q[state][action])\n",
    "        \n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c89d2ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State (0, 0): best action 2 (Q=[7.213021139647562, 6.1619250720739736, 8.971743736164308])\n",
      "State (0, 1): best action 2 (Q=[7.489111172475347, 6.461366888753406, 9.586523835541822])\n",
      "State (0, 2): best action 2 (Q=[7.38309940846315, 6.8336835757242556, 9.062064742811907])\n",
      "State (1, 0): best action 0 (Q=[10.872861886349263, 6.5420771441203955, 7.78888145610285])\n",
      "State (1, 1): best action 0 (Q=[11.845432865893033, 6.727180372290738, 8.014759464678434])\n",
      "State (1, 2): best action 2 (Q=[7.16056082771801, 6.411007252464571, 9.074740317583512])\n",
      "State (2, 0): best action 1 (Q=[11.022480814508775, 12.108206071762718, 8.357044802264127])\n",
      "State (2, 1): best action 0 (Q=[10.991240865283928, 6.454442287063826, 8.261927115876809])\n",
      "State (2, 2): best action 2 (Q=[7.5184592655859515, 6.893539675943709, 9.138608284266493])\n"
     ]
    }
   ],
   "source": [
    "for state, actions in Q.items():\n",
    "    best = np.argmax(actions)\n",
    "    print(f\"State {state}: best action {best} (Q={actions})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9b772",
   "metadata": {},
   "source": [
    "## 2. policy based\n",
    "\n",
    "**Goal**: Parameterize the policy, π(a | s; θ), by a set of weights θ (e.g., a neural network). The network takes the state s as input and outputs a probability distribution over actions.\n",
    "\n",
    "**How it Works**: The agent runs a batch of episodes (e.g., a full simulation of colony growth). It then tweaks the policy parameters θ to make good actions (those that led to high total reward in the episode) more probable, and bad actions less probable.\n",
    "[REINFORCE Algorithm](https://dilithjay.com/blog/reinforce-a-quick-introduction-with-code)\n",
    "\n",
    "The core update is: θ_new ← θ_old + α * ∇J(θ)\n",
    "Where ∇J(θ) is the policy gradient, an estimate of the direction in parameter space that will increase the expected reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec02b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -13.00\n",
      "Episode 200, Total Reward: 20.00\n",
      "Episode 400, Total Reward: 21.00\n",
      "Episode 600, Total Reward: 21.00\n",
      "Episode 800, Total Reward: 26.00\n",
      "\n",
      "Learned Policy (action probabilities):\n",
      "State (0, 0): P(actions) = ['0.005', '0.001', '0.993'], best action: 2\n",
      "State (0, 1): P(actions) = ['0.006', '0.006', '0.988'], best action: 2\n",
      "State (0, 2): P(actions) = ['0.005', '0.002', '0.993'], best action: 2\n",
      "State (1, 0): P(actions) = ['0.994', '0.002', '0.004'], best action: 0\n",
      "State (1, 1): P(actions) = ['0.996', '0.002', '0.002'], best action: 0\n",
      "State (1, 2): P(actions) = ['0.004', '0.002', '0.994'], best action: 2\n",
      "State (2, 0): P(actions) = ['0.006', '0.991', '0.003'], best action: 1\n",
      "State (2, 1): P(actions) = ['0.996', '0.001', '0.003'], best action: 0\n",
      "State (2, 2): P(actions) = ['0.006', '0.003', '0.991'], best action: 2\n",
      "\n",
      "Testing learned policy:\n",
      "State (1, 0) -> Action 0 -> Reward 2 -> Next state (1, 0)\n",
      "State (1, 0) -> Action 0 -> Reward 2 -> Next state (1, 1)\n",
      "State (1, 1) -> Action 0 -> Reward 2 -> Next state (0, 1)\n",
      "State (0, 1) -> Action 2 -> Reward 0 -> Next state (1, 2)\n",
      "State (1, 2) -> Action 2 -> Reward 0 -> Next state (1, 1)\n",
      "State (1, 1) -> Action 0 -> Reward 2 -> Next state (1, 1)\n",
      "State (1, 1) -> Action 0 -> Reward 2 -> Next state (0, 0)\n",
      "State (0, 0) -> Action 2 -> Reward 0 -> Next state (2, 2)\n",
      "State (2, 2) -> Action 2 -> Reward 0 -> Next state (2, 1)\n",
      "State (2, 1) -> Action 0 -> Reward 2 -> Next state (2, 0)\n",
      "Total test reward: 12\n"
     ]
    }
   ],
   "source": [
    "from agents.manualPolicyGradient import manualPolicyGradient\n",
    "policy = manualPolicyGradient()\n",
    "\n",
    "# Training with policy gradients\n",
    "env = minimumColonyEnv()\n",
    "num_episodes = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    episode_rewards = []\n",
    "    episode_probs = []\n",
    "    \n",
    "    # Run episode\n",
    "    for t in range(20):\n",
    "        action, action_probs = policy.choose_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        episode_states.append(state)\n",
    "        episode_actions.append(action)\n",
    "        episode_rewards.append(reward)\n",
    "        episode_probs.append(action_probs)\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    # Update policy after episode\n",
    "    policy.update(episode_states, episode_actions, episode_rewards, learning_rate)\n",
    "    \n",
    "    # Print progress\n",
    "    if episode % 200 == 0:\n",
    "        total_reward = sum(episode_rewards)\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "# Display learned policy\n",
    "print(\"\\nLearned Policy (action probabilities):\")\n",
    "for state in [(n, p) for n in [0,1,2] for p in [0,1,2]]:\n",
    "    probs = policy.get_action_probs(state)\n",
    "    best_action = np.argmax(probs)\n",
    "    print(f\"State {state}: P(actions) = {[f'{p:.3f}' for p in probs]}, best action: {best_action}\")\n",
    "\n",
    "# Test the learned policy\n",
    "print(\"\\nTesting learned policy:\")\n",
    "state = env.reset()\n",
    "total_test_reward = 0\n",
    "for t in range(10):\n",
    "    action, probs = policy.choose_action(state)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    print(f\"State {state} -> Action {action} -> Reward {reward} -> Next state {next_state}\")\n",
    "    total_test_reward += reward\n",
    "    state = next_state\n",
    "print(f\"Total test reward: {total_test_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a909a",
   "metadata": {},
   "source": [
    "### 2.1 Key Differences from Value-Based Approach:  \n",
    "No Q-table: Instead of storing action values, we store policy parameters theta that define action probabilities.\n",
    "\n",
    "- Action Selection:\n",
    "\n",
    "    - Value-based: action = argmax(Q[state]) (deterministic based on values)\n",
    "\n",
    "    - Policy-based: action = sample from π(a|state) (stochastic based on probabilities)\n",
    "\n",
    "- Learning Mechanism:\n",
    "\n",
    "    - Value-based: Directly update Q-values using Bellman equation\n",
    "\n",
    "    - Policy-based: Update policy parameters in the direction that increases expected reward\n",
    "\n",
    "- Output:\n",
    "\n",
    "    - Value-based: Shows which action has highest value in each state\n",
    "\n",
    "    - Policy-based: Shows probability distribution over actions for each state\n",
    "\n",
    "#### Advantages for Colony Project:\n",
    "1. Natural Stochasticity: Bacteria don't always make the same decision in the same state - policy gradients capture this biological reality.\n",
    "\n",
    "2. Continuous Action Spaces: Easy to extend to continuous actions (like growth rate).\n",
    "\n",
    "3. Better Credit Assignment: Directly links actions to long-term outcomes through the episode returns.\n",
    "\n",
    "4. Multi-agent Friendly: Each agent can have its own stochastic policy, which often works better in multi-agent settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e95c4",
   "metadata": {},
   "source": [
    "### 2.2 why logits\n",
    "logit p = $\\sigma^{-1}(p) = ln \\frac{p}{1-p}$ for $ p \\in (0,1)$ \n",
    "\n",
    "#### Why Use Logits Instead of Probabilities?\n",
    "1. Numerical Stability in Gradient Updates:  \n",
    "If we stored probabilities directly, the gradient update would be messy: We'd have to ensure probabilities stay positive and sum to 1. This requires complex constrained optimization\n",
    "\n",
    "2. Avoid Probability Boundary Issues:  \n",
    "If we stored probabilities and one probability became very small (say 0.001), it would be hard to \"recover\" that action if it later became good. With logits, even if the softmax probability is tiny, the logit can still be updated freely.\n",
    "\n",
    "3. Better Gradient Behavior:  \n",
    "The gradient of the log probability with respect to logits has a nice property:  \n",
    "$\\nabla log \\pi(a|s) = (1 - π) \\text{for chosen action, } -\\pi $ for others  \n",
    "This gradient is well-behaved and doesn't vanish when probabilities approach 0 or 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e53226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Logits:\n",
      "Initial logits: [0.0, 0.0, 0.0]\n",
      "Probabilities: [0.33333333 0.33333333 0.33333333]\n",
      "\n",
      "With Direct Probabilities:\n",
      "Probabilities: [0.33, 0.33, 0.34]\n",
      "\n",
      "--- After preferring action 0 ---\n",
      "Logits: [5.0, 0.0, 0.0]\n",
      "Probabilities: [0.98670329 0.00664835 0.00664835]\n",
      "Probabilities: [0.98, 0.01, 0.01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PolicyWithLogits:\n",
    "    def __init__(self):\n",
    "        self.theta = {}\n",
    "        for n in [0, 1, 2]:\n",
    "            for p in [0, 1, 2]:\n",
    "                self.theta[(n, p)] = [0.0, 0.0, 0.0]  # Logits\n",
    "    \n",
    "    def get_action_probs(self, state):\n",
    "        \"\"\"Convert logits to probabilities using softmax\"\"\"\n",
    "        logits = self.theta[state]\n",
    "        exp_logits = np.exp(logits - np.max(logits))  # Numerical stability\n",
    "        probs = exp_logits / np.sum(exp_logits)\n",
    "        return probs\n",
    "\n",
    "class PolicyWithDirectProbs:\n",
    "    def __init__(self):\n",
    "        self.theta = {}\n",
    "        for n in [0, 1, 2]:\n",
    "            for p in [0, 1, 2]:\n",
    "                self.theta[(n, p)] = [0.33, 0.33, 0.34]  # Direct probabilities\n",
    "    \n",
    "    def get_action_probs(self, state):\n",
    "        \"\"\"Already have probabilities, just return them\"\"\"\n",
    "        return self.theta[state]\n",
    "\n",
    "# Compare the two approaches\n",
    "policy_logits = PolicyWithLogits()\n",
    "policy_direct = PolicyWithDirectProbs()\n",
    "\n",
    "state = (1, 1)\n",
    "print(\"With Logits:\")\n",
    "print(f\"Initial logits: {policy_logits.theta[state]}\")\n",
    "print(f\"Probabilities: {policy_logits.get_action_probs(state)}\")\n",
    "\n",
    "print(\"\\nWith Direct Probabilities:\")\n",
    "print(f\"Probabilities: {policy_direct.get_action_probs(state)}\")\n",
    "\n",
    "# After an update that strongly prefers action 0:\n",
    "print(\"\\n--- After preferring action 0 ---\")\n",
    "\n",
    "# With logits - can represent strong preferences\n",
    "policy_logits.theta[state] = [5.0, 0.0, 0.0]  # Strong preference for action 0\n",
    "print(f\"Logits: {policy_logits.theta[state]}\")\n",
    "print(f\"Probabilities: {policy_logits.get_action_probs(state)}\")\n",
    "\n",
    "# With direct probabilities - limited by boundaries\n",
    "policy_direct.theta[state] = [0.98, 0.01, 0.01]  # Can't go much higher\n",
    "print(f\"Probabilities: {policy_direct.get_action_probs(state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeede7a",
   "metadata": {},
   "source": [
    "### 2.3 Why softmax not simple conversion to probabilities\n",
    "softmax: $\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} $  \n",
    "Why simple conversion fails:\n",
    "- Logits can be negative - probabilities would become negative (invalid!)\n",
    "- No amplification of differences - a logit of [1, 2, 3] would give similar probabilities to [100, 200, 300]\n",
    "- No probabilistic interpretation - the outputs don't represent true probabilities\n",
    "\n",
    "Why softmax:\n",
    "1. Amplifies Relative Differences\n",
    "\n",
    "2. Perfect for Gradient-Based Learning\n",
    "The gradient of softmax has a beautiful property for policy gradients\n",
    "The gradient ∇logπ(a|s) becomes very simple:  \n",
    "For chosen action: (1 - π)  \n",
    "For other actions: (-π)  \n",
    "This makes the update rule clean and efficient\n",
    "\n",
    "3. Interpretable as Probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b9b916",
   "metadata": {},
   "source": [
    "### 2.4.1 more on REINFORCE algorithm\n",
    "[REINFORCE algorithm — Reinforcement Learning from scratch in PyTorch](https://medium.com/@sofeikov/reinforce-algorithm-reinforcement-learning-from-scratch-in-pytorch-41fcccafa107) \n",
    "\n",
    "#### 1. Policy Representation (by deepseek)\n",
    "\n",
    "The policy is parameterized by $\\theta$ and defines a probability distribution over actions:\n",
    "\n",
    "$ \\pi(a|s; \\theta) = softmax(\\theta[s,a']) = \\frac{\\exp(\\theta[s,a])}{\\sum_{a'} \\exp(\\theta[s,a'])} $\n",
    "\n",
    "Where:\n",
    "- $\\pi(a|s; \\theta)$ is the probability of taking action $a$ in state $s$\n",
    "- $\\theta[s,a]$ are the logits for state $s$ and action $a$\n",
    "\n",
    "#### 2. Objective Function\n",
    "\n",
    "We want to maximize the expected return:\n",
    "\n",
    "$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\gamma^t R_t \\right]\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $\\gamma \\in [0,1]$ is the discount factor\n",
    "- $R_t$ is the reward at time $t$\n",
    "- $\\tau = (s_0, a_0, R_1, s_1, a_1, R_2, \\ldots)$ is a trajectory\n",
    "\n",
    "#### 3. Policy Gradient Theorem\n",
    "\n",
    "The gradient of the objective is:\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi(a_t|s_t; \\theta) \\cdot G_t \\right]\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $G_t = \\sum_{k=t}^T \\gamma^{k-t} R_{k+1}$ is the discounted return from time $t$\n",
    "- $\\nabla_\\theta \\log \\pi(a_t|s_t; \\theta)$ is the score function\n",
    "\n",
    "#### 4. Score Function Gradient\n",
    "\n",
    "For the softmax policy, the gradient has a simple form:\n",
    "\n",
    "$\n",
    "\\nabla_\\theta \\log \\pi(a|s; \\theta) = \\mathbf{1}_a - \\pi(s; \\theta)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{1}_a$ is a one-hot vector with 1 at position $a$ and 0 elsewhere\n",
    "- $\\pi(s; \\theta)$ is the vector of action probabilities in state $s$\n",
    "\n",
    "#### 5. Monte Carlo Estimate\n",
    "\n",
    "We approximate the expectation by sampling $N$ trajectories:\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\left[ \\mathbf{1}_{a_t^i} - \\pi(s_t^i; \\theta) \\right] \\cdot G_t^i\n",
    "$\n",
    "\n",
    "#### 6. Parameter Update Rule\n",
    "\n",
    "$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\cdot \\nabla_\\theta J(\\theta)\n",
    "$\n",
    "\n",
    "Where $\\alpha$ is the learning rate.\n",
    "\n",
    "#### 7. Complete REINFORCE Algorithm\n",
    "\n",
    "For each episode $i$:\n",
    "1. Generate trajectory: $\\tau^i = \\{(s_0^i, a_0^i, R_1^i), (s_1^i, a_1^i, R_2^i), \\ldots, (s_T^i, a_T^i, R_{T+1}^i)\\}$\n",
    "2. For $t = 0$ to $T$:\n",
    "   $\n",
    "   G_t^i = \\sum_{k=t}^T \\gamma^{k-t} R_{k+1}^i\n",
    "   $\n",
    "3. For $t = 0$ to $T$:\n",
    "   $\n",
    "   \\theta \\leftarrow \\theta + \\alpha \\cdot \\gamma^t \\cdot G_t^i \\cdot \\left[ \\mathbf{1}_{a_t^i} - \\pi(s_t^i; \\theta) \\right]\n",
    "   $\n",
    "\n",
    "#### 8. With Return Normalization\n",
    "\n",
    "To reduce variance, we normalize returns:\n",
    "\n",
    "$\n",
    "\\hat{G}_t^i = \\frac{G_t^i - \\mu_G}{\\sigma_G + \\epsilon}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $\\mu_G = \\frac{1}{N \\cdot T} \\sum_{i=1}^N \\sum_{t=0}^T G_t^i$\n",
    "- $\\sigma_G = \\sqrt{\\frac{1}{N \\cdot T} \\sum_{i=1}^N \\sum_{t=0}^T (G_t^i - \\mu_G)^2}$\n",
    "- $\\epsilon = 10^{-8}$ (small constant for numerical stability)\n",
    "\n",
    "#### 9. Per-State Update Rule\n",
    "\n",
    "For a specific state-action pair $(s, a)$:\n",
    "\n",
    "$\n",
    "\\theta[s,a] \\leftarrow \\theta[s,a] + \\alpha \\cdot \\hat{G}_t \\cdot (1 - \\pi(a|s; \\theta))\n",
    "$\n",
    "\n",
    "For other actions $a' \\neq a$:\n",
    "\n",
    "$\n",
    "\\theta[s,a'] \\leftarrow \\theta[s,a'] + \\alpha \\cdot \\hat{G}_t \\cdot (0 - \\pi(a'|s; \\theta))\n",
    "$\n",
    "\n",
    "#### 10. Learning Interpretation\n",
    "\n",
    "- **When $\\hat{G}_t > 0$**: Increase $\\pi(a_t|s_t)$, decrease $\\pi(a'|s_t)$ for $a' \\neq a_t$\n",
    "- **When $\\hat{G}_t < 0$**: Decrease $\\pi(a_t|s_t)$, increase $\\pi(a'|s_t)$ for $a' \\neq a_t$\n",
    "- **Update magnitude**: Proportional to $|\\hat{G}_t|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336fa2c",
   "metadata": {},
   "source": [
    "### 2.4.2 Derivation of Policy Gradient Update Rule\n",
    "\n",
    "##### Step 1: Objective Function\n",
    "\n",
    "We want to maximize the expected return:\n",
    "\n",
    "$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\gamma^t R(s_t, a_t) \\right]\n",
    "$\n",
    "\n",
    "Where a trajectory $\\tau = (s_0, a_0, s_1, a_1, \\ldots, s_T)$ has probability:\n",
    "\n",
    "$\n",
    "P(\\tau|\\theta) = P(s_0) \\prod_{t=0}^T \\pi(a_t|s_t; \\theta) P(s_{t+1}|s_t, a_t)\n",
    "$\n",
    "\n",
    "##### Step 2: Express Expectation Explicitly\n",
    "\n",
    "$\n",
    "J(\\theta) = \\sum_\\tau P(\\tau|\\theta) \\left( \\sum_{t=0}^T \\gamma^t R(s_t, a_t) \\right)\n",
    "$\n",
    "\n",
    "##### Step 3: Take Gradient\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\left[ \\sum_\\tau P(\\tau|\\theta) \\left( \\sum_{t=0}^T \\gamma^t R(s_t, a_t) \\right) \\right]\n",
    "$\n",
    "\n",
    "$\n",
    "= \\sum_\\tau \\nabla_\\theta P(\\tau|\\theta) \\left( \\sum_{t=0}^T \\gamma^t R(s_t, a_t) \\right)\n",
    "$\n",
    "\n",
    "##### Step 4: Log-Derivative Trick\n",
    "\n",
    "Use the identity: $\\nabla_\\theta P(\\tau|\\theta) = P(\\tau|\\theta) \\nabla_\\theta \\log P(\\tau|\\theta)$\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\sum_\\tau P(\\tau|\\theta) \\nabla_\\theta \\log P(\\tau|\\theta) \\left( \\sum_{t=0}^T \\gamma^t R(s_t, a_t) \\right)\n",
    "$\n",
    "\n",
    "##### Step 5: Expand Log-Probability of Trajectory\n",
    "\n",
    "$\n",
    "\\log P(\\tau|\\theta) = \\log P(s_0) + \\sum_{t=0}^T \\left[ \\log \\pi(a_t|s_t; \\theta) + \\log P(s_{t+1}|s_t, a_t) \\right]\n",
    "$\n",
    "\n",
    "$\n",
    "\\nabla_\\theta \\log P(\\tau|\\theta) = \\sum_{t=0}^T \\nabla_\\theta \\log \\pi(a_t|s_t; \\theta)\n",
    "$\n",
    "\n",
    "The dynamics terms $P(s_{t+1}|s_t, a_t)$ and initial state $P(s_0)$ don't depend on $\\theta$.\n",
    "\n",
    "##### Step 6: Substitute Back\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\sum_\\tau P(\\tau|\\theta) \\left( \\sum_{t=0}^T \\nabla_\\theta \\log \\pi(a_t|s_t; \\theta) \\right) \\left( \\sum_{k=0}^T \\gamma^k R(s_k, a_k) \\right)\n",
    "$\n",
    "\n",
    "##### Step 7: Policy Gradient Theorem\n",
    "\n",
    "We can rewrite this as:\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi(a_t|s_t; \\theta) \\left( \\sum_{k=t}^T \\gamma^k R(s_k, a_k) \\right) \\right]\n",
    "$\n",
    "\n",
    "Or more commonly:\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi(a_t|s_t; \\theta) \\cdot G_t \\right]\n",
    "$\n",
    "\n",
    "Where $G_t = \\sum_{k=t}^T \\gamma^{k-t} R_{k+1}$ is the return from time $t$.\n",
    "\n",
    "##### Step 8: Softmax Policy Gradient\n",
    "\n",
    "For a softmax policy:\n",
    "\n",
    "$\n",
    "\\pi(a|s; \\theta) = \\frac{\\exp(\\theta[s,a])}{\\sum_{a'} \\exp(\\theta[s,a'])}\n",
    "$\n",
    "\n",
    "The gradient of the log-policy is:\n",
    "\n",
    "$\n",
    "\\nabla_\\theta \\log \\pi(a|s; \\theta) = \\frac{\\nabla_\\theta \\pi(a|s; \\theta)}{\\pi(a|s; \\theta)}\n",
    "$\n",
    "\n",
    "Compute the gradient for a specific parameter $\\theta[s,a]$:\n",
    "\n",
    "$\n",
    "\\frac{\\partial}{\\partial \\theta[s,a]} \\log \\pi(a'|s; \\theta) = \n",
    "\\begin{cases}\n",
    "1 - \\pi(a|s; \\theta) & \\text{if } a' = a \\\\\n",
    "-\\pi(a|s; \\theta) & \\text{if } a' \\neq a\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "##### Step 9: Vector Form\n",
    "\n",
    "In vector form for all actions:\n",
    "\n",
    "$\n",
    "\\nabla_\\theta \\log \\pi(a|s; \\theta) = \\mathbf{1}_a - \\pi(s; \\theta)\n",
    "$\n",
    "\n",
    "Where $\\mathbf{1}_a$ is a one-hot vector with 1 at position $a$.\n",
    "\n",
    "##### Step 10: Final Update Rule\n",
    "\n",
    "Combining everything, we get the REINFORCE update:\n",
    "\n",
    "$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\cdot \\gamma^t \\cdot G_t \\cdot \\left[ \\mathbf{1}_{a_t} - \\pi(s_t; \\theta) \\right]\n",
    "$\n",
    "\n",
    "##### Step 11: Monte Carlo Approximation\n",
    "\n",
    "In practice, we approximate the expectation by sampling:\n",
    "\n",
    "$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\cdot \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\gamma^t G_t^i \\cdot \\left[ \\mathbf{1}_{a_t^i} - \\pi(s_t^i; \\theta) \\right]\n",
    "$\n",
    "\n",
    "##### Key Insights:\n",
    "\n",
    "1. **The dynamics disappear**: We don't need to know $P(s_{t+1}|s_t, a_t)$\n",
    "2. **Only need trajectories**: We can learn from experience alone\n",
    "3. **Credit assignment**: Each action gets credit for subsequent rewards\n",
    "4. **Variance reduction**: Using $G_t$ instead of total return reduces variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
